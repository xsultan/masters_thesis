% -*- root: cuthesis_masters.tex -*-
\section{Introduction}
\label{chap4:sec:introduction}
\setlength\parindent{24pt} 

\revision In the previous chapter, we studied the impact of \SATD on software quality and found that the presence of SATD leads to more difficult changes in the future. Supplementing these comment-based indicators with the metric-based indicators used in earlier work \cite{zazworka2011investigating} (God Classes), we replicate the study conducted in chapter 3 on a larger scale in order to compare the impact of comment- and metric-based technical debt on quality. Of the foregoing studies that have covered how metric-based technical debt affects software quality, few have done so on large datasets. We remedy this on the one hand by integrating both comment- and metric-based approaches, as mentioned, and on the other by introducing a more granular analysis on the file and change levels. Empirically examining the impact of both approaches on software quality and comparing any differences between them will provide researchers and practitioners with a more global understanding of technical debt, warn them of its future risks and raise awareness of the challenges it can pose.

Predictably, as technical debt has become a more popular strategy at developers' disposal, numerous advances have been made in detecting it, some metric-based and others comment-based. The former includes Marinescu's \cite{marinescu2004detection} methodology, which detects \textit{God Class} code smells according to sets of rules and thresholds defined on various object-oriented metrics. The latter, advocated by Potdar and Shihab's \cite{ICSM_PotdarS14} methodology in recent work, flags recurring source code comment patterns that correlate with incidence of \textit{self-admitted technical debt} (SATD). Moreover, the nature of the comments that developers leave has allowed occurrences of SATD to be sub-categorized and analyzed accordingly.

Without access to research that treats technical debt from all angles, developers will be misinformed as to the costs and benefits of technical debt and unequipped to decide responsibly whether it should be assumed in a given scenario---not to mention lacking effective strategies for keeping it in check once assumed. Our work closes this gap as we study 40 open-source projects that bring into focus the empirical links between both \SATD and god classes and software quality. If our results are true for a larger number of projects, then they are even more likely to generalize to others. 

Our inquiry pursues: (i) whether god class and SATD files contribute more defects than files free of god classes and SATD, (ii) whether emerging defects can be traced to god and SATD changes, (iii) whether god- and SATD-related changes are associated with greater difficulty and (iv) to what extent the comment- and metric-based approaches identify the same instances of technical debt. As in the previous chapter, amount of churn, quantity of affected files and modified modules and change entropy all factor into the change difficulty calculations. In the end we observed that: (i) no straightforward correlation exists between incidence of SATD or god files and incidence of defects, (ii) more future defects surfaced after performing god and SATD changes than non-god and non-SATD changes and (iii) god and SATD changes are more difficult to perform than non-god and non-SATD changes. Preliminarily, (i)-(iii) concede that the downsides of god classes and \SATD are increases in future defect density and change difficulty. Yet the two approaches were found to reinforce each other in that (iv) between 11\% and 34\% of technical debt sources were identified by both the comment- and metric-based approaches.


%\section{Related Work}
%\label{chap4:sec:related_work}
%\todo{only mention the new studies, what have they found, talk about your study \& how it differs}
%Our work uses code comments and object-oriented metrics  to identify technical debt. Therefore, we divide the related work into four categories: source code comments, technical debt, software quality, and identifying and detecting code smells.
%\subsection{Source Code Comments}
%Developers have tapped into the many advantages of source code comments in prioritizing, delegating and finalizing personal and team tasks~\cite{Storey:2008}. Another benefit has to do with how comments contribute to a greater understanding of the programs with which they are affiliated~\cite{TakangGM96}; without them, pinpointing program defects quickly becomes a ``needle in a haystack" kind of scenario. In order to avoid such situations, developers must monitor code-comment consistency, ensuring that changes in the code are updated in the comments and vice versa, and to this end, Tan \textit{et al.} recommend @iComment for detecting lock- and call-related inconsistencies~\cite{tan07icomment}, @aComment for interrupt context synchronization inconsistencies~\cite{acomment} and @tComment for inconsistencies emanating from exceptions to inferred Javadoc properties~\cite{tcomment}. Even so, code-comment inconsistencies are not as prevalent as one might expect, Fluri \textit{et al.}~\cite{fluri2007code} having determined the rate of co-changed code and comments to be 97\%.

%Malik \textit{et al.}~\cite{malik2008understanding} identify the rate of changed call dependencies and control statements, modified function age and the amount of co-changed dependent functions as the foremost attributes leading to source code comment updates. Given the importance of keeping comments consistent with high-level artifacts and stabilizing source code identifiers, De Lucia \textit{et al.}~\cite{DeLucia2011} conducted a study that verified---through a series of controlled experiments---that a resemblance between source code and high-level software artifacts does improve comment and identifier quality.

%Source code comments are also salient in the arena of \SATD detection, as Potdar and Shihab~\cite{ICSM_PotdarS14} demonstrated in researching to what extent and for what reasons SATD is introduced and how often it is subsequently removed. In fact, comments served as the basis of Maldonado and Shihab's~\cite{MTD15p9} SATD classification system and allowed them to quantify the incidence of the five types they identified. At a rate of 42\% to 84\% of 33,000 classified comments, design debt was easily the most common.

%\subsection{Technical Debt}

%Several major studies have centered on technical debt besides SATD---technical debt that source code analysis tools are deployed to detect. Zazworka \textit{et al.}~\cite{Zazworka:2013}, weighing the merits of automated and manual detection strategies, concur that the returns on engineering more tools and heuristics capable of detecting technical debt would greatly advance the field as it now stands. Looking at how god classes influence maintainability later in the development process, Zazworka \textit{et al.}~\cite{zazworka2011investigating} established that god classes are changed more frequently and harbor more defects than their non-god counterparts and thus that the undesirable ramifications of technical debt might extend to software quality. Guo \textit{et al.}~\cite{GuoSGCTSSS11} monitored the lifecycle of one delayed task and specified how some of the consequences of technical debt surface in real time.

%\subsection{Software Quality}
%Studies in the same vein as~\cite{Zimmerman2008Springer} have been devoted to outlining the factors that either do detract or could detract from software quality and raising awareness among project contributors. However, metrics, too, have been used to isolate defects, including: design and code~\cite{Jiang-promise-2008}, code churn~\cite{Nagappan-icse-2005} and process metrics~\cite{Moser-icse-2008,Rahman-icse-2013}.

%The likes of the SZZ algorithm originated by Sliwerski \textit{et al.}~\cite{Sliwerski-fse-2005} and Kim \textit{et al.}'s~\cite{Kim-tse-2008} defect-proneness identifiers excerpted from change logs and added or deleted source code mark a shift in defect prediction towards the change level. The former links a version archive to a bug database in order to isolate fix-inducing changes~\cite{Sliwerski-fse-2005}, whereas Kamei's~\cite{Kamei-tse-2013} ``Just-In-Time Quality Assurance" approach exemplifies how process metrics outdo product metrics in identifying risky software changes.

%\subsection{Identifying and Detecting Code Smells}
%Fowler and Beck \cite{fowler1999refactoring} originated the term \textit{code smell} to designate various indicators of object-oriented design flaws which can undermine software maintenance. Code smells respond to the internal and external properties of the system elements they monitor. Though manual code smell detection warns developers of potential vulnerabilities, Marinescu \cite{Marinescu_ICETOOLS} observes that it is time-consuming, non-repeatable and non-scalable. Apart from this, the more familiar the software system is to a developer, the higher the risk of a subjective appraisal of its efficiencies and shortcomings, according to Mäntylä \cite{mantyla2003taxonomy, mantyla2004bad}, and one important corollary of this is that a developer's chances of overlooking design flaws increase. In order to surmount these drawbacks, Marinescu recommends enlisting code metrics to detect system volatilities, and in this spirit, several implementations of this departure from manual detection have been devised \cite{lanza2007object, marinescu2004detection, Marinescu_PhD, Marinescu_IBM_JRD}.

\section{Approach}
\label{chap4:sec:approach}
As we continue to study the interplay between \SATD and metric-based debt (God Classes) and software quality, measures must be established in order to quantify software quality \cite{Kamei-tse-2013,Kim-tse-2008,sliwerski-msr-2005}. The precedent in accomplishing this task has been to count the defects in SATD files and calculate the rate of future defect introduction among SATD changes, expressed as a percentage. Deferring to the technical debt metaphor and its concept of accruing ``interest" to be paid in the long run, we also measure software quality in terms of SATD change difficulty, which we calculate as stipulated earlier. With these metrics standardized, we entertain the research questions that follow:

\begin{itemize}
	\vspace{0.1cm}
	\item {\bf RQ1:} Do god and SATD files have more defects than non-god and non-SATD files?
	%\vspace{0.1cm}
	\item {\bf RQ2:} Do god- and SATD-related changes introduce future defects?
	%\vspace{0.2cm}
	\item {\bf RQ3:} Are god- and SATD-related changes more difficult than non-god and non-SATD changes?
	%\vspace{0.2cm}
	\item {\bf RQ4:} Is there an overlap between comment- and metric-based technical debt? 
\end{itemize}

We devised a suitable methodology, visualized in Figure \ref{fig:CH4_Process_overview}, to guide our inquiry into these questions. We initiate the process by mining the source code repositories and pulling source code files on a project-by-project basis (steps 1-2). Afterwards the source code files are parsed and comments extracted (step 3). We then identify all instances of \SATD, count defects file-wide and isolate defect-inducing changes by means of the SZZ algorithm (steps 4-5).

\begin{figure}[h]
	\centering
	\includegraphics[width=150mm]{figures/chapter4/approach}
	\caption{Metric-based (God Classes) approach overview.}
	\label{fig:CH4_Process_overview}
\end{figure}


\subsection{Data Extraction}

To generalize the scope of this study, we relied on certain criteria in selecting the 40 open-source software projects: (i) well-commented source code, (ii) varying numbers of lines of code (LOC), (iii) different numbers of contributors, (iv) different development domains, (v) mature development history and (vi) issue tracking system capability. The first criterion is a prerequisite for Potdar and Shihab's SATD detection technique; the last is essential to accurately study the introduction of future defects.

Given the extent to which our approach to locating \SATD relies on source code comments, we downloaded the most recent versions of the relevant systems, filtered this input to generate source code and excluded all files lacking source code comments (\eg{} CSS, XML, JSON). As for comments suspected of indicating no SATD, e.g. license comments, commented source code, Javadoc comments, etc., four filtering heuristics were deployed to remove them from the results.

Tables \ref{table:ch4_projects_statistics} and \ref{table:ch4_projects_statistics_2} showcase some key identifiers and statistics for each project, including: (i) which release was downloaded, (ii) the number of lines of code it contains, (iii) the number of comment lines, (iv) a source code file count, (v) the number of committers in the project's development history and (vi) its commit count.

\begin{landscape}


\begin{table}[htbp]
	\small
	\centering
	\caption{Characteristics of the studied projects (part 1).}
	\begin{adjustbox}{width=1.3\textwidth}


		\begin{tabular}{l|l|c|c|c|c|c}
			\hline
			\textbf{Project}           & \textbf{Release} & \textbf{\# Lines of Code} & \textbf{\# Comment Lines} & \textbf{\# Files} & \textbf{\# Committers} & \textbf{\# Commits} \\ \hline
			\textbf{Apache OpenNLP}    & 1.7.1            &          163,114           &           30,581           &        861        &           11           &        1,339         \\ \hline
			\textbf{Apache Camel}      & 2.18.0           &          1,626,040          &          430,818           &       17,046       &          333           &        25,461        \\ \hline
			\textbf{Apache Habse}      & ‎1.2.4           &          1,310,985          &          251,409           &       3,243        &          166           &        12,531        \\ \hline
			\textbf{Apache Groovy}     & 2.4.2            &          286,920           &           85,885           &       1,517        &          262           &        13,422        \\ \hline
			\textbf{Apache Oltu}       & 1.0.2            &           267,88           &           7,386            &        300        &           14            &         842         \\ \hline
			\textbf{Apache Maven}      & 3.3.9            &          150,724           &           34,830           &       1,540        &           81           &        10,370        \\ \hline
			\textbf{Apache Karaf}      & 5.4.0            &          155,675           &           32,906           &       1,467        &           86           &        5,666         \\ \hline
			\textbf{Apache Hama}       & 0.6.4            &           855,64           &           22,545           &        499        &           22           &        1,592         \\ \hline
			\textbf{Apache Tomee}      & 1.7.4            &          854,611           &          212,542           &       6,297        &           35           &        10,257        \\ \hline
			\textbf{Apache Deltaspike} & 1.7.2            &          149,871           &           45,722           &       1,842        &           48           &        2,044         \\ \hline
			\textbf{Apache Curator}    & 2.10.0           &          124,077           &           18,458           &        525        &           66           &        1,813         \\ \hline
			\textbf{Apache Calcite}    & 1.10.0           &          448,820           &          110,410           &       1,702        &          102           &        2,180         \\ \hline
			\textbf{Apache Poi}        & 3.15             &          644,284           &          182,823           &       3,298        &           39           &        7,963         \\ \hline
			\textbf{Apache Zeppelin}   & 0.6.2            &          124,865           &           15,314           &        552        &          201           &        2,642         \\ \hline
			\textbf{Apache Ant}        & 1.9.7            &          343,010           &          109,150           &       1,927        &           62           &        13,425        \\ \hline
			\textbf{Apache Stanbol}    & 1.0.0            &          339,699           &          107,208           &       2,044        &           25           &        3,398         \\ \hline
			\textbf{Apache Kafka}      & 0.10.1           &          152,685           &           33,368           &       1,002        &          300           &        2,734         \\ \hline
			\textbf{Apache Tika}       & 1.13             &          142,786           &           38,984           &        992        &           54           &        3,209         \\ \hline
			\textbf{Apache Felix}      & ‎5.4.0           &          837,955           &          211,404           &       5,039        &           51           &        13,240        \\ \hline
			\textbf{Apache Phoenix}    & 4.9.0            &          396,054           &           69,326           &       1,620        &           59           &        1,748         \\ \hline
		\end{tabular}
		\label{table:ch4_projects_statistics}

\end{adjustbox}

\end{table}

\end{landscape}



\begin{landscape}

	
	
	\begin{table}[htbp]
		\small
		\centering
		\caption{Characteristics of the studied projects (part 2).}
		\begin{adjustbox}{width=1.3\textwidth}
			
			
			\begin{tabular}{l|l|c|c|c|c|c}
				\hline
				\textbf{Project}           & \textbf{Release} & \textbf{\# Lines of Code} & \textbf{\# Comment Lines} & \textbf{\# Files} & \textbf{\# Committers} & \textbf{\# Commits} \\ \hline
				\textbf{Apache Wicket}     & 7.5.9            & 548,923                    &          191,279           &       4,830        &           80           &        19,574        \\ \hline
				\textbf{Apache Aurora}     & 0.16.0           & 317,551                    &           70,774           &       1,284        &          105           &        3,639         \\ \hline
				\textbf{Apache Ignite}     & 1.6              & 1,498,260                   &          443,157           &       7,361        &          117           &        17,933        \\ \hline
				\textbf{Apache Helix}      & 0.7.1            & 141,632                    &           34,430           &        900        &           32           &        2,266         \\ \hline
				\textbf{Apache Archiva}    & 2.2.1            & 191,743                    &           33,641           &       1,172        &           41           &        7,742         \\ \hline
				\textbf{Apache Struts}     & 2.5              & 316,495                    &           83,911           &       2,307        &           65           &        4,634         \\ \hline
				\textbf{Apache Derby}      & 10.13.1.1        & 1,271,629                   &          386,703           &       3,023        &           37           &        8,127         \\ \hline
				\textbf{Apache Ambari}     & ‎2.4.2           & 2,220,418                   &          421,937           &       10,223       &          119           &        18,025        \\ \hline
				\textbf{Apache Nifi}       & 1.0.0            & 576,512                    &          118,806           &       3,493        &          120           &        2,826         \\ \hline
				\textbf{Apache Tiles}      & 3.0.7            & 51,487                     &           20,173           &        599        &           16           &        1,455         \\ \hline
				\textbf{Apache Shiro}      & ‎1.3.2           & 81,131                     &           36,854           &        727        &           22           &        1,641         \\ \hline
				\textbf{Apache Usergrid}   & 2.1.0            & 605,286                    &          114,999           &       2,619        &          110           &        10,621        \\ \hline
				\textbf{Apache Nutch}      & 2.3              & 104,214                    &           27,478           &        843        &           37           &        2,217         \\ \hline
				\textbf{Apache Zookeeper}  & 3.4.9            & 196,008                    &           38,867           &        814        &           21           &        1,468         \\ \hline
				\textbf{Apache Mina}       & 2.0.16           & 45,588                     &           14,336           &        340        &           29           &        2,400         \\ \hline
				\textbf{Apache Cxf}        & 3.1.8            & 988,585                    &          196,520           &       8,806        &           81           &        12,302        \\ \hline
				\textbf{Apache CloudStack} & 4.9.0            & 1,423,346                   &          207,036           &       6,424        &          412           &        29,931        \\ \hline
				\textbf{Apache Oozie}      & 4.3.0RC0         & 256,423                    &           49,510           &       1,239        &           22           &        1,772         \\ \hline
				\textbf{Apache Kylin}      & 1.5.4.1          & 217,645                    &           45,320           &       1,227        &           89           &        5,121         \\ \hline
				\textbf{Apache Flink}      & 1.1.2            & 791,670                    &          195,738           &       4,154        &          325           &        9,513         \\ \hline
			\end{tabular}
			\label{table:ch4_projects_statistics_2}
			
		\end{adjustbox}
		
	\end{table}
	
\end{landscape}


\subsection{Scanning Code and Extracting Comments}

Now source code comments must be extracted from the projects under investigation, for which we implement a Python-based tool. The rest of the extraction process is the same as for subsection \ref{ch3_scanningCodeAndExtracting}. 

%Having isolated the source code from all of the projects under investigation, we implemented a Python-based tool to extract the source code comments. Beyond this, the tool reveals each comment's type (i.e., single-line or block comments), the name of its host file and its line number. The Count Lines of Code (CLOC) tool ~\cite{cloc} double checks the Python-based tool, and provided that there is no discrepancy between the total number of lines of comments both yield, we have confidence in the accuracy of the tool we developed.

\subsection{Filter Comments}


Source code comments left by developers might situate the project within the circumstances of its development, communicate their recommendations for revising the code at a later date, acknowledge who wrote which pieces or who made which fixes or confess that self-admitted technical debt has been assumed. Efforts should be made to decrease the volume of comments, especially when sifting through them in search of \SATD confessions. For this reason, we make use of several filtering heuristics proposed by Moldonado \textit{et al.} \cite{Maldonado_TSE2017} to focus our search query.\par

A Python-based tool reads data retrieved from parsed source code, initiates the filtering heuristics and stores the results in the database. The retrieved data specify each class or comment's starting and ending line numbers as well as Java syntax comment type (i.e., single-line, block or Javadoc). Once this information is acquired, the filtering heuristics are processed. \par

Self-admitted technical debt is seldom indicated in comments left prior to class declaration, e.g. license comments, among others, so we benefit from any mechanism that identifies and omits such distractors without also omitting comments that incorporate Java IDE task annotations (i.e., ``TODO:", ``FIXME:" or ``XXX:"). If a comment features any of these keywords, tasks related to the comment will be added to an IDE-generated list for ease of access. As for separating pre- and post-class declaration comments, the number of the line in which the class is declared marks the crucial cutoff in that any preceding comments are targeted for removal.\par

Comment type matters insofar as cumbersome comments stitched together from single-line comment components (and not block comments) impede message interpretation for comments read one by one. A heuristic that pinpoints and collapses sequences of adjacent single-line comments into block comments overcomes the comment type issue.\par



Commented source code does not indicate \SATD in our experience, but rather either code not used at all or code used exclusively for debugging. To eliminate this distractor, we use a regular expression to remove typical Java code structures, i.e., public, private, for, exception, etc.\par


Most IDEs auto-generate comments when creating a method, constructor, try catch, etc. Due to the nature of the auto-generation of these comments, there is no SATD content. The majority of Javadoc comments also fail to mention SATD, and those that do are annotated with at least one task (i.e., ``TODO:", ``FIXME:", ``XXX:"). This criterion allows our heuristic to determine which Javadoc comments should be salvaged versus ignored, while no distinction is necessary for auto-generated comments. We designed a regular expression to apply the criterion by checking for task annotations before omitting the comment.\par




The procedure was conceived with the intention of factoring out the contribution of noise, which ultimately improves the quality of the comment dataset by reducing cases of SATD false positives and prioritizing the most applicable comments.\par



\subsection{Identifying Self-Admitted Technical Debt}
\label{ch4_td}


Our analysis hinges on locating \SATD at two levels: (i) the file level and (ii) the change level.

\noindent\textbf{SATD files:}
We emulated Potdar and Shihab \cite{ICSM_PotdarS14} in identifying \SATD on the basis of 62 different patterns recurring in multiple projects at various frequencies. The specifics of these patterns can be found in subsection \ref{ch3_td}.


\noindent\textbf{SATD changes:}
At the change level, all the files touched by the same change are checked for evidence of \SATD. If any one of them is determined to be an SATD file, the whole change is classified as an SATD change. Alternatively, if none of the files touched by a change is an SATD file, the whole change falls into the non-SATD change category. In general, the more SATD files a system contains, the more likely it is to have a higher number of SATD changes. SATD comments are shown to account for less than 6.10\% and SATD files for somewhere between 1.37\% and 25.03\% of the respective totals for all systems in Tables~\ref{table:projects_satd_god_percentage} and \ref{table:projects_satd_god_percentage2}, where each system's percentages are listed separately for comparison.

\begin{landscape}
	
	
	\begin{table}[htbp]
		\small
		\centering
		\caption{Percentage of SATD and God of the analyzed projects (part 1).}
		\begin{adjustbox}{width=1.0\textwidth}
			
			
			\begin{tabular}{l|c|c|c}
				\hline
				\textbf{Project}  & \textbf{SATD Comments (\%)}  & \textbf{SATD Files (\%)} & \textbf{God Files (\%)} \\ \hline
				\textbf{Apache OpenNLP} &  2.66 & 19.02 & 14.77   \\ \hline
				\textbf{Apache Camel} &  0.95 & 3.53 & 12.95    \\ \hline
				\textbf{Apache Habse} & 1.69 &  18.29 & 15.05    \\ \hline
				\textbf{Apache Groovy} &  3.41 & 13.67 & 14.52    \\ \hline
				\textbf{Apache Oltu} &  1.91 & 5.67 & 14.27    \\ \hline
				\textbf{Apache Maven} &  3.76 & 10.65 & 13.92    \\ \hline
				\textbf{Apache Karaf} &  2.40 & 7.44 & 14.58   \\ \hline
				\textbf{Apache Hama} &  1.44 & 11.24 & 14.48    \\ \hline
				\textbf{Apache Tomee} &  1.94 & 7.16 & 14.33   \\ \hline
				\textbf{Apache Deltaspike} &  3.95 & 9.28 & 9.26    \\ \hline
				\textbf{Apache Curator} &  0.99 & 5.34 & 15.32    \\ \hline
				\textbf{Apache Calcite} &  1.67 & 12.97 & 14.54    \\ \hline
				\textbf{Apache Poi} &  2.11 & 16.79 & 14.43    \\ \hline
				\textbf{Apache Zeppelin} &  1.62 & 9.37 & 14.87    \\ \hline
				\textbf{Apache Ant} &  2.23 & 20.56 & 14.60   \\ \hline
				\textbf{Apache Stanbol} &  4.03 & 25.03 & 14.06   \\ \hline
				\textbf{Apache Kafka} &  1.56 & 7.73 & 11.71    \\ \hline
				\textbf{Apache Tika} &  3.29 & 19.28 & 14.40    \\ \hline
				\textbf{Apache Felix} & 1.88 & 9.72 & 13.59   \\ \hline
				\textbf{Apache Phoenix} &  3.34 & 13.81 & 13.47    \\ \hline
				
			\end{tabular}
			\label{table:projects_satd_god_percentage}
			
		\end{adjustbox}
		
	\end{table}
	
\end{landscape}



\begin{landscape}
	
	
	
	\begin{table}[htbp]
		\small
		\centering
		\caption{Percentage of SATD and God of the analyzed projects  (part 2).}
		\begin{adjustbox}{width=1.0\textwidth}
			
			
			\begin{tabular}{l|c|c|c}
				\hline
				\textbf{Project}  & \textbf{SATD Comments (\%)}  & \textbf{SATD Files (\%)} & \textbf{God Files (\%)} \\ \hline
				\textbf{Apache Wicket} &  0.89 & 5.29 & 11.52    \\ \hline
				\textbf{Apache Aurora} &  3.97 & 14.27 & 14.91    \\ \hline
				\textbf{Apache Ignite} &  0.26 & 3.12 & 13.50   \\ \hline
				\textbf{Apache Helix} &  4.02 & 18.09 & 15.62    \\ \hline
				\textbf{Apache Archiva} &  6.05 & 17.71 & 12.61    \\ \hline
				\textbf{Apache Struts} &  1.85 & 8.07 & 12.39    \\ \hline
				\textbf{Apache Derby} &  1.20 & 22.66 & 14.37    \\ \hline
				\textbf{Apache Ambari} &  2.29 & 8.76 & 13.31   \\ \hline
				\textbf{Apache Nifi} &  0.87 & 4.62 & 13.29    \\ \hline
				\textbf{Apache Tiles} &  0.17 & 1.37 & 10.90    \\ \hline
				\textbf{Apache Shiro} &  2.67 & 15.86 & 12.85    \\ \hline
				\textbf{Apache Usergrid} &  2.20 & 11.20 & 12.33    \\ \hline
				\textbf{Apache Nutch} &  2.17 & 12.25 & 15.60    \\ \hline
				\textbf{Apache Zookeeper} &  1.71 & 10.31 & 13.82   \\ \hline
				\textbf{Apache Mina} &  1.12 & 5.99 & 14.71   \\ \hline
				\textbf{Apache Cxf} &  2.78 & 6.87 & 14.04   \\ \hline
				\textbf{Apache CloudStack} &  1.98 & 10.42 & 14.04   \\ \hline
				\textbf{Apache Oozie} &  1.63 & 9.73 & 15.81   \\ \hline
				\textbf{Apache Kylin} &  1.64 & 8.06 & 15.38   \\ \hline
				\textbf{Apache Flink} &  0.57 & 3.72 & 14.84   \\ \hline
				
			\end{tabular}
			\label{table:projects_satd_god_percentage2}
			
		\end{adjustbox}
		
	\end{table}
	
\end{landscape}


\subsection{God Classes}
God classes are classes that combine trivial class workloads and generally avoid assigning tasks to other classes. They are distinguishable on account of their high complexity, low inner-class cohesion and frequent foreign class data access \cite{lanza2007object}. Object-oriented design advocates a one-to-one correspondence between classes and responsibilities, which god classes violate by definition \cite{lanza2007object}. Due to their size and the extent to which they are tied to other classes, god classes can make it more difficult to understand the system \cite{fowler1999refactoring} and are expected to be more susceptible to defects during system maintenance. The higher the incidence of defects, the more often changes will have to be performed and the bigger those changes will be, compounding maintenance over time \cite{fowler1999refactoring, lanza2007object}.

\subsection{Identifying God Classes}
\label{ch4_god}

To perform our analysis, we need to identify god classes at the same two levels as \SATD: (i) the file level and (ii) the change level.

\noindent\textbf{God files:} To identify god classes, we followed the methodology outlined by Marinescu \cite{marinescu2004detection}, who proposed an approach to specify and detect
code smells, specifically \textit{God Classes}. Their technique leverages metric-based heuristics that identify god classes according to sets of rules and thresholds defined on various object-oriented metrics. The formula provided below in Figure \ref{equation:1} operates on three metrics---namely, weighted method count (WMC), tight class cohesion (TCC) and access to foreign data (ATFD)---and generates one of two outputs. If the output is 1, then the class to which the formula is applied is a god class; if 0, it is a non-god class.

\noindent\textbf{God changes:}
To study the impact of god classes at the change level, we must first identify which classes are god classes and which are non-god classes. By analogy with the technique used to identify SATD and non-SATD changes, we consider any change containing at least one god file to be a god change and those containing no god files to be non-god changes. 


\begin{figure}[h]
\[GodClass(C) = \left\{\begin{matrix}
1& (AFTD(C), HigherThan(1))  \wedge ((WMC(C), TopValues(25\%)) \vee \\ 
 & (TCC, BottomValues(25\%)))\\ 
0& else
\end{matrix}\right.\]
\captionsetup[figure]{list=no}
\caption{God Class Detection Equation}
\label{equation:1}
\end{figure}

The equation in Figure~\ref{equation:1} above describes how we detect a god class, where:


\begin{itemize}
\item[$\bullet$] \textbf{Weighted Method Count (WMC)} is the sum of the statistical complexity of all methods in a class \cite{Chidamber_Kemerer_94}. McCabe's cyclomatic complexity \cite{McCabe_1976} is used as a complexity measure for all class methods.
\item[$\bullet$] \textbf{Tight Class Cohesion (TCC)} is the number of directly connected public methods in a class \cite{Bieman:1995:CRO:223427.211856}.
\item[$\bullet$] \textbf{Access to Foreign Data (ATFD)} is the number of external classes whose attributes are accessed either directly or indirectly (by accessor methods) \cite{Marinescu_PhD}.
\end{itemize}

\subsection{Identifying Defects in God Files and God Changes}
\label{ch4_bugs_td}

According to Sliwersky \textit{et al.} \cite{sliwerski-msr-2005}, expressions denoting defect identifiers, e.g. ``\textit{fixed issue, bug ID, fix, defect, patch, crash, freeze, breaks, wrong, glitch, properly, proper}," ordinarily certify that an earlier mistake has been corrected when recorded in control system change logs. Other work has proposed comparable methodologies for tracking fault-inducing changes until repaired \cite{Kamei-tse-2013, Kim-tse-2008, sliwerski-msr-2005}. Next we pull each defect report from its corresponding issue tracking system, i.e., Bugzilla~\footnote{https://www.bugzilla.org} or JIRA~\footnote{https://www.atlassian.com/software/jira}, and comb for all pertinent details. Once the god files and god changes have been separated, we go about determining the number of file defects and identifying any defect-inducing changes in the same way foregoing research has ~\cite{Kamei-tse-2013, Kim-tse-2008, sliwerski-msr-2005}.

\noindent\textbf{Defects in files:}
A file defect count is a prerequisite to any defectiveness comparison between god and non-god files. With this in mind, we first view a file's history and extract all the changes that have touched it. The list this produces is then shortened as change log searches return only results consistent with keywords indicating corrective changes.
Examples of these keywords can be found in subsection \ref{ch3_bugs}, along with the steps we take to rule out false positives.


\noindent \textbf{Defect-inducing changes:}
Along the same lines, we search the commit messages using regular expressions that convey defect fixes as a means of establishing whether a given change is corrective. The keywords used to identify corrective changes are given in subsection \ref{ch3_bugs}, where the procedure for identifying defect-inducing changes is presented.

\section{Case Study Results}
\label{chap4:sec:case_study_results}

In this section, we present the empirical outcomes of our inquiry into the correlation between both self-admitted technical debt and god classes and software quality. Each of the three research questions is restated below, where we summarize its motivation, our approach in treating it and the conclusions we reached. Statistics and results are listed for all individual projects, accompanied by cross-project comparisons.

%\sultan{stopped here}
\subsection*{\chapterIVrqI}


\noindent{\textbf{Motivation:}}
Reluctance to resort to code smells and technical debt suggests that most developers believe these adversely affect software quality, and what research has been conducted supports this conviction~\cite{zazworka2011investigating}. %\sultan{I'm not sure how to incorporate god classes in this case since there has been a lot of work that studied their impact on software quality}
The potential drawbacks of SATD, meanwhile, have remained unexplored despite its research-affirmed ubiquity in software projects \cite{ICSM_PotdarS14}. Researchers and developers should be better equipped to negotiate the long-term risks of SATD and have empirical evidence in hand that identifies concrete issues its use can bring about, raising SATD literacy within the community at large.


\noindent{\textbf{Approach:}}
We compare god files versus non-god files and SATD files versus non-SATD files in terms of defect-proneness.


\noindent\textbf{Comparing god and non-god files:}
The God Class Detection Equation proposed by \cite{marinescu2004detection} provides a way to identify god classes using object-oriented metrics. Files are fed to the equation and are labeled either god or non-god files, depending on the output. We calculate the percentage of defect-fixing changes for each file in both categories based on $\frac{\left (\frac{\#~of~fixing~changes}{total~\#~changes} \right )}{SLOC}$. We normalize our data since files can have different amounts of changes and apply a test designed to measure whether the differences between the categories are statistically significant.

In case this distribution is non-normal, we use the non-parametric Mann-Whitney~\cite{mann1947test} test because, unlike the parametric alternatives, it is capable of handling such distributions. A \textit{p}-value such that $p \le 0.05$ indicates that the difference between the samples is statistically significant.


\noindent\textbf{Comparing SATD and non-SATD files:}
We identify SATD files in accordance with the procedure detailed in section~\ref{ch3_td}, labeling files containing any number of SATD comments as SATD files and all others non-SATD files. These two file categories (SATD and non-SATD) undergo calculations yielding the percentage of defect-fixing changes, $\frac{\left (\frac{\#~of~fixing~changes}{total~\#~changes} \right )}{SLOC}$ , which, unlike pure counts, standardizes the metric across files hosting different numbers of changes. Afterwards the defect distribution is plotted for SATD and non-SATD files and a test is performed to uncover any statistical trends.

Again, we elected to conduct the non-parametric Mann-Whitney~\cite{mann1947test} test to decide whether a statistical difference exists between the SATD and non-SATD groups rather than a parametric substitute, which could not accommodate non-normal distribution. A statistically significant difference returns a \textit{p}-value of at most 0.05 ($p \le 0.05$).


\noindent{\textbf{Results - Defects in god files vs. non-god files and SATD files vs. non-SATD files:}}


\begin{figure}[h]
	\centering
	\includegraphics[width=140mm]{figures/chapter4/rq1_defectivness_distrubution_new}
	\caption{Percentage of defect-fixing changes for (i) God vs. non-God files and (ii) SATD vs. NSATD files.}
	\label{figure:ch4_number_of_fixing_changes_TD_vs_NTD}
\end{figure}

Beanplots are a convenient way to present and compare univariate data for two groups. Those in Figure \ref{figure:ch4_number_of_fixing_changes_TD_vs_NTD} display the distribution of \textit{median corrective change rates}, based on $\frac{\left (\frac{\#~of~fixing~changes}{total~\#~changes} \right )}{SLOC}$, for god files versus non-god files and SATD files versus non-SATD files \textit{in each project}. %These rates represent the number of bug-fixing changes over the number of total changes.
The dotted lines represent the overall mean taking data from both groups into account; for this reason, the dotted lines mark the same value on both sides of each beanplot. The solid lines, in contrast, mark the median value for each group and thus differ from one side to the other. A comparison of \textit{distribution medians} indicates that the defectiveness rates for god and SATD files are lower than the corresponding rates for non-god and non-SATD files.

\begin{myboxii}[RQ1 - Interim Summary]
	Neither god files nor \SATD files are associated with a higher percentage of defects.
\end{myboxii}

Figure~\ref{figure:percentage_of_defects_god_vs_ngod} (in the appendix) shows boxplots for the individual projects which compare the distribution of defectiveness rates for god and non-god files. We can see that the rate is higher for non-god files than god files within each project and that this difference is statistically significant such that $p \le 0.05$, which holds when all project medians are consolidated in the distribution in Figure \ref{figure:ch4_number_of_fixing_changes_TD_vs_NTD}.

Although Figure \ref{figure:ch4_number_of_fixing_changes_TD_vs_NTD} indicates that non-SATD files have a higher median defectiveness rate than SATD files, this trend is not borne out in every individual project. In Figure \ref{figure:percentage_of_defects_td_vs_ntd}, we observe that OpenNLP, Curator and Tiles constitute exceptions where the defectiveness rate is higher among SATD files.


\subsection*{\chapterIVrqII}


\noindent{\textbf{Motivation:}}
Having looked at how god and non-god and SATD and non-SATD compare at the file level, we turn our sights to the question of whether god and SATD changes introduce future defects at a higher rate than non-god and non-SATD changes. Before, entire files were the objects of our analysis; now we require an analysis tailored to assess individual changes.


To investigate how change category relates to introduction of future defects and the duration of the ``grace period" before god and SATD changes impact software quality, we should first determine to what extent god classes and SATD are predisposed to introduce future defects. Our conjecture is that god and SATD changes introduce future defects at a higher rate than non-god and non-SATD changes. Further, we expect that if a god or SATD change causes a defect to be introduced in the change right after, then the delay is minimal and the impact on quality cannot be put off for very long.


\noindent{\textbf{Approach:}}
We identify defect-inducing changes utilizing the SZZ algorithm~\cite{sliwerski-msr-2005} and subdivide the results it generates into four categories depending on whether the changes contain god classes or not (god versus non-god defect-inducing changes) and whether they contain SATD or not (SATD versus non-SATD defect-inducing changes). We then apply the Mann-Whitney test \cite{mann1947test} to evaluate the statistical significance of the difference between god versus non-god defect-inducing changes and SATD versus non-SATD defect-inducing changes (from the same respective data sets). If the resulting \textit{p}-value is such that $p \le 0.05$, then the difference is not attributable to chance but rather statistically significant, and generalizes to other data sets.


\noindent{\textbf{Results:} God and non-god distributions of defect-inducing change rates in each project share a common vertical axis in the first plot in Figure \ref{figure:ch4_bug_inducing_changes}, as do SATD and non-SATD defect-inducing change rate distributions in the second. We observe that the distribution median (i.e., the median of the individual project medians) is higher for the god and SATD changes than for the non-god and non-SATD changes. This indicates that god and SATD changes have more of a tendency to induce future defects than their non-god and non-SATD counterparts. We also find that the differences between god versus non-god and SATD versus non-SATD defect-inducing changes are both statistically significant with $p \le 0.05$.


\begin{myboxii}[RQ2 - Interim Summary]
	Both god changes and SATD changes tend to lead to a higher number of future defects.
\end{myboxii}

\begin{figure}[h]
	\centering
	\includegraphics[width=140mm]{figures/chapter4/rq2_new}
	\caption{Percentage of defect-inducing changes for (i) God vs. non-God and (ii) SATD vs. NSATD.}
	\label{figure:ch4_bug_inducing_changes}
\end{figure}

As a general rule, what is true of the distribution medians is also true of individual project medians, though exceptions exist---among them Apache Hbase, Apache Kafka, Apache Mina, Apache Shiro, Apache Oozie, Apache Flink, Apache Deltaspike and Apache curator in Figures \ref{figure:percentage_of_bug_inducing_god_vs_ngod} and \ref{figure:percentage_of_bug_inducing_td_vs_ntd}. In these projects, the non-god and non-SATD changes appeared to induce more future defects than the god and SATD changes. These isolated counterexamples, while in conflict with the trend observed in Figure \ref{figure:ch4_bug_inducing_changes}, are compatible with our findings in chapter 3, where we report that SATD changes have less of a tendency to induce future defects.

\subsection*{\chapterIVrqIII}

\noindent{\textbf{Motivation:}}
Up to this point, we have been concentrating on the interplay between both god classes and SATD and defects lowering software quality. As we know from previous research \cite{marinescu2004detection}, god classes violate object-oriented design principles and have negative long-term implications for system maintainability. Likewise, if we recall the ramifications of the technical debt metaphor, we see that the short-term payoff should come at an increased cost later on in development. The deferred consequences of god classes and technical debt are measured in terms of increasing difficulty, which has yet to be fully examined after detecting god classes and introducing technical debt. Verifying that god classes and SATD do increase change difficulty will better portray their implications for future changes and software projects and in the end enable developers to see the full picture when deciding whether or not to refactor god classes or introduce \SATD. \\

\noindent{\textbf{Approach:}}
We recognize god, non-god, SATD and non-SATD change categories and compare the difficulty of executing changes from each category. We reuse the four metrics from chapter 3 to determine change difficulty: churn, the number of modified directories, the number of modified files and the entropy of the change. For a slightly different purpose, Eick \emph{et al.}~\cite{eick2001decay} chose the first three to measure decay; Hassan~\cite{hassan2009predicting} utilized the last metric to measure change complexity. As in RQ2, we use the Mann-Whitney test \cite{mann1947test} to determine whether the differences between god and non-god changes and between SATD and non-SATD changes are statistically significant and measure the effect size using Cliff's delta \cite{Cliff:2005}, this time with respect to the complexity metric categories. 


%To measure the change churn, number of files and number of directories, we extract these metrics from the change log. To calculate churn for a whole change, we add up the number of lines inserted and deleted in each individual file touched by the change. To count the files and directories touched by a change, we extract the list of files from the change log. We consider a directory to be \textbf{ND} and a file to be \textbf{NF} when measuring the number of modified directories and files. Thus, if a change involves the modification of a file having the path ``src/os/unix/ngx\_alloc.h," then the directory is \textit{src/os/unix} and the file is \textit{ngx\_alloc.h}.\\



\begin{figure}[!hp]
	\centering
	\includegraphics[width=120mm]{figures/chapter4/rq3_churn}
	\caption{Total number of lines modified per change for (i) God vs. non-God and (ii) SATD vs. NSATD.}
	\label{figure:ch4_tlcpc}
\end{figure}



\begin{figure}[!hp]
	\centering
	\includegraphics[width=120mm]{figures/chapter4/rq3_nf}
	\caption{Total number of files modified per change for (i) God vs. non-God and (ii) SATD vs. NSATD.}
	\label{figure:ch4_tfcpc}
\end{figure}

\begin{figure}[!hp]
	\centering
	\includegraphics[width=120mm]{figures/chapter4/rq3_nd}
	\caption{Total number of modified directories per change for (i) God vs. non-God and (ii) SATD vs. NSATD.}
	\label{figure:ch4_number_of_directories}
\end{figure}



\begin{figure}[!hp]
	\centering
	\includegraphics[width=120mm]{figures/chapter4/rq3_entropy}
	\caption{Distribution of the change across files for (i) God vs. non-God and (ii) SATD vs. NSATD.}
	\label{figure:ch4_mtdocatdf}
\end{figure}



%We adopt Hassan's change complexity measure~\cite{hassan2009predicting} to calculate change entropy, defined as $H(P)=-\sum _{k=1}^{n}\textrm ({p}_{k}*{log}_{2} {p}_{k})$, where $k$ is the proportion file$_{k}$ is modified in a change and $n$ is the number of files in the change. Entropy measures the distribution of a change across files. If we consider a change involving modification of three files \textit{A, B} and \textit{C}, for example, and take the number of lines modified to be 30, 1 and 1, respectively, the entropy equates to $0.40=-\frac{30}{32}\log_{2}\frac{30}{32}-\frac{1}{32}\log_{2}\frac{1}{32}-\frac{1}{32}\log_{2}\frac{1}{32}$.

%The maximum entropy $log_{2}n$ has normalized the entropy formula presented above following Hassan~\cite{hassan2009predicting} so that entropy can be compared for changes touching different numbers of files. The higher the normalized entropy, the more difficult it is to make the change.\\

\noindent{\textbf{Results:}}
In each one of Figures~\ref{figure:ch4_tlcpc},~\ref{figure:ch4_tfcpc},~\ref{figure:ch4_number_of_directories} and~\ref{figure:ch4_mtdocatdf}, a distribution is compiled for god and non-god and SATD and non-SATD changes by plotting the median values obtained from a different difficulty measure for each project. Juxtaposition of the \textit{distribution medians} demonstrates that regardless of the metric employed to quantify change difficulty, god and SATD changes are consistently more difficult to perform than non-god and non-SATD changes. Moreover, the Mann-Whitney test \cite{mann1947test} yields $p \le 0.05$, indicating that the differences are statistically significant. Tables~\ref{table:cliff_deltas_RQ4_God} and ~\ref{table:cliff_deltas_RQ4_TD} show the Cliff's delta ~\cite{Cliff:2005} effect size values for all projects studied for both god and SATD changes. We observe that for most projects and all measures of difficulty, the effect size is either medium or large, except for \textit{Hbase, Hama, Deltaspike, Calcite and Stanbol} for god changes and \textit{Hbase and Deltaspike} for SATD changes, which have a small effect size.\\

\begin{landscape}
	\begin{table}[!htbp]

	\small
	\centering


	
	\begin{tabular}{l|c|c|c|c}
		\hline
		\textbf{Project}   & {\bf NF}    & {\bf E} & {\bf C} & {\bf ND}    \\ \hline
\textbf{Apache openNLP} & 0.31 & 0.30 & 0.35 & 0.36\\ \hline
\textbf{Apache Camel} & 0.42 & 0.41 & 0.39 & 0.37\\ \hline
\textbf{Apache Hbase} & 0.23 & 0.24 & 0.29 & 0.22\\ \hline
\textbf{Apache Groovy} & 0.36 & 0.33 & 0.34 & 0.40\\ \hline
\textbf{Apache Oltu} & 0.49 & 0.47 & 0.43 & 0.48\\ \hline
\textbf{Apache Maven} & 0.35 & 0.34 & 0.33 & 0.33\\ \hline
\textbf{Apache Karaf} & 0.29 & 0.26 & 0.36 & 0.27\\ \hline
\textbf{Apache Hama} & 0.27 & 0.25 & 0.32 & 0.22\\ \hline
\textbf{Apache Tomee} & 0.36 & 0.34 & 0.36 & 0.35\\ \hline
\textbf{Apache Deltaspike} & 0.30 & 0.28 & 0.29 & 0.29\\ \hline
\textbf{Apache Curator} & 0.22 & 0.33 & 0.45 & 0.33\\ \hline
\textbf{Apache Calcite} & 0.22 & 0.24 & 0.23 & 0.27\\ \hline
\textbf{Apache Poi} & 0.44 & 0.42 & 0.36 & 0.49\\ \hline
\textbf{Apache Zeppelin} & 0.44 & 0.42 & 0.44 & 0.46\\ \hline
\textbf{Apache Ant} & 0.47 & 0.45 & 0.43 & 0.49\\ \hline
\textbf{Apache Stanbol} & 0.27 & 0.25 & 0.30 & 0.24\\ \hline
\textbf{Apache Kafka} & 0.35 & 0.34 & 0.33 & 0.35\\ \hline
\textbf{Apache Tika} & 0.35 & 0.33 & 0.33 & 0.36\\ \hline
\textbf{Apache Felix} & 0.37 & 0.35 & 0.34 & 0.30\\ \hline
\textbf{Apache Phoenix} & 0.50 & 0.45 & 0.47 & 0.54\\ \hline
\end{tabular}
\quad \quad \quad 
	\begin{tabular}{l|c|c|c|c}
	\hline
	\textbf{Project}   & {\bf NF}    & {\bf E} & {\bf C} & {\bf ND}    \\ \hline

\textbf{Apache Wicket} & 0.58 & 0.58 & 0.55 & 0.53\\ \hline
\textbf{Apache Aurora} & 0.50 & 0.49 & 0.45 & 0.57\\ \hline
\textbf{Apache Ignite} & 0.52 & 0.39 & 0.52 & 0.53\\ \hline
\textbf{Apache Helix} & 0.45 & 0.42 & 0.47 & 0.43\\ \hline
\textbf{Apache Archiva} & 0.39 & 0.39 & 0.38 & 0.30\\ \hline
\textbf{Apache Struts} & 0.40 & 0.48 & 0.43 & 0.43\\ \hline
\textbf{Apache Derby} & 0.54 & 0.53 & 0.52 & 0.52\\ \hline
\textbf{Apache Ambari} & 0.34 & 0.32 & 0.41 & 0.33\\ \hline
\textbf{Apache Nifi} & 0.40 & 0.37 & 0.43 & 0.44\\ \hline
\textbf{Apache Tiles} & 0.30 & 0.29 & 0.29 & 0.36\\ \hline
\textbf{Apache Shiro} & 0.36 & 0.34 & 0.38 & 0.39\\ \hline
\textbf{Apache Usergrid} & 0.50 & 0.43 & 0.46 & 0.52\\ \hline
\textbf{Apache Nutch} & 0.32 & 0.34 & 0.31 & 0.40\\ \hline
\textbf{Apache Zookeeper} & 0.31 & 0.26 & 0.27 & 0.45\\ \hline
\textbf{Apache Mina} & 0.42 & 0.39 & 0.40 & 0.43\\ \hline
\textbf{Apache Cxf} & 0.56 & 0.54 & 0.56 & 0.55\\ \hline
\textbf{Apache Cloudstack} & 0.37 & 0.34 & 0.36 & 0.44\\ \hline
\textbf{Apache Oozie} & 0.52 & 0.47 & 0.47 & 0.60\\ \hline
\textbf{Apache Kylin} & 0.36 & 0.30 & 0.35 & 0.38\\ \hline
\textbf{Apache Flink} & 0.53 & 0.51 & 0.57 & 0.59\\ \hline



	\end{tabular}
		\caption{Cliff's delta for the change difficulty measures across the projects for God Changes.}
		\label{table:cliff_deltas_RQ4_God}
\end{table}
\end{landscape}


\begin{landscape}
	\begin{table}[!htbp]
		
		\small
		\centering
		
		
		
		\begin{tabular}{l|c|c|c|c}
			\hline
			\textbf{Project}   & {\bf NF}    & {\bf E} & {\bf C} & {\bf ND}    \\ \hline
			\textbf{Apache openNLP} & 0.34 & 0.33 & 0.36 & 0.39\\ \hline
			\textbf{Apache Camel} & 0.44 & 0.42 & 0.38 & 0.40
\\ \hline
			\textbf{Apache Hbase} & 0.26 & 0.27 & 0.29 & 0.24
\\ \hline
			\textbf{Apache Groovy} & 0.35 & 0.32 & 0.33 & 0.39
\\ \hline
			\textbf{Apache Oltu} & 0.36 & 0.33 & 0.41 & 0.35\\ \hline
			\textbf{Apache Maven} & 0.40 & 0.39 & 0.37 & 0.38
\\ \hline
			\textbf{Apache Karaf} & 0.35 & 0.32 & 0.40 & 0.34
\\ \hline
			\textbf{Apache Hama} & 0.34 & 0.31 & 0.37 & 0.28
\\ \hline
			\textbf{Apache Tomee} & 0.35 & 0.33 & 0.34 & 0.34
\\ \hline
			\textbf{Apache Deltaspike} & 0.31 & 0.29 & 0.29 & 0.26
\\ \hline
			\textbf{Apache Curator} & 0.40 & 0.30 & 0.51 & 0.40
\\ \hline
			\textbf{Apache Calcite} & 0.30 & 0.31 & 0.29 & 0.35
\\ \hline
			\textbf{Apache Poi} & 0.48 & 0.47 & 0.37 & 0.51\\ \hline
			\textbf{Apache Zeppelin} & 0.59 & 0.57 & 0.56 & 0.59
\\ \hline
			\textbf{Apache Ant} & 0.41 & 0.49 & 0.47 & 0.49
\\ \hline
			\textbf{Apache Stanbol} & 0.33 & 0.30 & 0.34 & 0.39
\\ \hline
			\textbf{Apache Kafka} & 0.40 & 0.39 & 0.39 & 0.40
\\ \hline
			\textbf{Apache Tika} & 0.37 & 0.33 & 0.30 & 0.38\\ \hline
			\textbf{Apache Felix} & 0.30 & 0.38 & 0.36 & 0.33
\\ \hline
			\textbf{Apache Phoenix} & 0.52 & 0.47 & 0.44 & 0.56\\ \hline
		\end{tabular}
		\quad \quad \quad 
		\begin{tabular}{l|c|c|c|c}
			\hline
			\textbf{Project}   & {\bf NF}    & {\bf E} & {\bf C} & {\bf ND}    \\ \hline
			
			\textbf{Apache Wicket} & 0.47 & 0.47 & 0.40 & 0.43
\\ \hline
			\textbf{Apache Aurora} & 0.52 & 0.51 & 0.48 & 0.59
\\ \hline
			\textbf{Apache Ignite} & 0.53 & 0.60 & 0.53 & 0.56
\\ \hline
			\textbf{Apache Helix} & 0.53 & 0.51 & 0.50 & 0.49
\\ \hline
			\textbf{Apache Archiva} & 0.30 & 0.39 & 0.39 & 0.40
\\ \hline
			\textbf{Apache Struts} & 0.37 & 0.39 & 0.39 & 0.34
\\ \hline
			\textbf{Apache Derby} & 0.37 & 0.36 & 0.32 & 0.34
\\ \hline
			\textbf{Apache Ambari} & 0.38 & 0.36 & 0.44 & 0.38
\\ \hline
			\textbf{Apache Nifi} & 0.57 & 0.55 & 0.53 & 0.59
\\ \hline
			\textbf{Apache Tiles} & 0.57 & 0.56 & 0.43 & 0.52
\\ \hline
			\textbf{Apache Shiro} & 0.35 & 0.39 & 0.35 & 0.36
\\ \hline
			\textbf{Apache Usergrid} & 0.51 & 0.44 & 0.47 & 0.52
\\ \hline
			\textbf{Apache Nutch} & 0.49 & 0.51 & 0.43 & 0.54
\\ \hline
			\textbf{Apache Zookeeper} & 0.39 & 0.35 & 0.33 & 0.47
\\ \hline
			\textbf{Apache Mina} & 0.54 & 0.49 & 0.47 & 0.51
\\ \hline
			\textbf{Apache Cxf} & 0.36 & 0.34 & 0.31 & 0.35 \\ \hline
			\textbf{Apache Cloudstack} & 0.40 & 0.37 & 0.29 & 0.46
\\ \hline
			\textbf{Apache Oozie} & 0.53 & 0.50 & 0.45 & 0.57
\\ \hline
			\textbf{Apache Kylin} & 0.57 & 0.52 & 0.53 & 0.59
\\ \hline
			\textbf{Apache Flink} & 0.47 & 0.44 & 0.39 & 0.49 \\ \hline
			
			
			
		\end{tabular}
		\caption{Cliff's delta for the change difficulty measures across the projects for SATD Changes.}
		\label{table:cliff_deltas_RQ4_TD}
	\end{table}
\end{landscape}

\begin{myboxii}[RQ3 - Interim Summary]
	God class-related changes and SATD-related changes are more difficult to perform than non-god and non-SATD changes.
\end{myboxii}

Upon inspection of the boxplots in Figures~\ref{figure:total_number_of_lines_changed_god_vs_ngod},~\ref{figure:total_number_of_lines_changed_td_vs_ntd},~\ref{figure:total_nd_changed_god_vs_ngod},~\ref{figure:total_nd_changed_td_vs_ntd},~\ref{figure:total_files_god_vs_ngod},~\ref{figure:total_files_td_vs_ntd},~\ref{figure:total_entropy_god_vs_ngod} and~\ref{figure:total_entropy_td_vs_ntd} (in appendix), we find that the distribution median and distribution mean inequalities remain unchanged for all projects for all difficulty measures except for number of directories in OpenNLP, where non-god and non-SATD change medians are still not greater than those of god and SATD changes, but about equal. In summary, then, performing god and SATD changes is more difficult than performing non-god and non-SATD changes by all difficulty measures.


\subsection*{\chapterIVrqIV}

%\todo{finish this research question}


\noindent{\textbf{Motivation:}}
Thus far, we have compared the impact of technical debt identified by comment- and metric-based approaches on software quality. What remains outstanding now is the amount of overlap between the \SATD files that the comment-based approach labels and the god files that the metric-based approach detects. Specifically, our objective is to calculate the percentage of files that contain technical debt on both counts so that we can garner a better understanding of how comment-based technical debt complements metric-based technical debt.


\noindent{\textbf{Approach:}}
We take the list of \SATD files generated by the comment-based approach and the list of god files generated by the metric-based approach and isolate the files that made both lists. We count how many of these files meet the criteria for both approaches and then divide by the total number of files in both lists. The result represents the share of comment-based technical debt that complements metric-based technical debt (overlap), expressed as a percentage.


\noindent{\textbf{Results:}}

Table~\ref{ch4_amount_of_overlap} displays the percentage overlap by project. We see that the values range from 11\% to 34\%. Our findings confirm that the comment-based approach, which uses source code comment patterns to detect technical debt, complements the metric-based approach, which relies on thresholds of object-oriented metrics. In other work, Moldonado \textit{et al.} \cite{Maldonado_TSE2017} studied the overlap between \SATD and several types of code smells extracted by a static analysis tool for 10 open-source projects. For \SATD and god classes specifically, they found an average overlap of 44.2\%, which, though higher than the overlap we found, is based on a smaller sample size. Despite considerable overlap, each of the comment- and metric-based approaches identifies some additional sources of technical debt that the other fails to detect.

\begin{myboxii}[RQ4 - Interim Summary]
	The comment-based approach complements the metric-based approach with an overlap ranging from 11\% to 34\%.
\end{myboxii}

	\begin{table}[!htbp]
	\small
	\centering
	
	\begin{tabular}{l|c}
		\hline
		\textbf{Project}           & \textbf{Overlap (\%)} \\ \hline
		\textbf{Apache OpenNLP}    &    24.39     \\ \hline
		\textbf{Apache Camel}      &    16.57     \\ \hline
		\textbf{Apache Habse}      &    33.88     \\ \hline
		\textbf{Apache Groovy}     &    30.10     \\ \hline
		\textbf{Apache Oltu}       &    16.68     \\ \hline
		\textbf{Apache Maven}      &    26.08     \\ \hline
		\textbf{Apache Karaf}      &    23.65     \\ \hline
		\textbf{Apache Hama}       &    29.51     \\ \hline
		\textbf{Apache Tomee}      &    21.61     \\ \hline
		\textbf{Apache Deltaspike} &    20.46     \\ \hline
		\textbf{Apache Curator}    &    25.23     \\ \hline
		\textbf{Apache Calcite}    &    31.12     \\ \hline
		\textbf{Apache Poi}        &    31.36     \\ \hline
		\textbf{Apache Zeppelin}   &    23.37     \\ \hline
		\textbf{Apache Ant}        &    28.86     \\ \hline
		\textbf{Apache Stanbol}    &    32.95     \\ \hline
		\textbf{Apache Kafka}      &    20.64     \\ \hline
		\textbf{Apache Tika}       &    33.56     \\ \hline
		\textbf{Apache Felix}      &    26.00     \\ \hline
		\textbf{Apache Phoenix}    &    30.99     \\ \hline
	\end{tabular}
	\quad \quad \quad
	\begin{tabular}{l|c}
		\hline
		\textbf{Project}           & \textbf{Overlap (\%)} \\ \hline
		\textbf{Apache Wicket}     &    19.33     \\ \hline
		\textbf{Apache Aurora}     &    26.44     \\ \hline
		\textbf{Apache Ignite}     &    14.28     \\ \hline
		\textbf{Apache Helix}      &    25.45     \\ \hline
		\textbf{Apache Archiva}    &    30.33     \\ \hline
		\textbf{Apache Struts}     &    24.74     \\ \hline
		\textbf{Apache Derby}      &    32.56     \\ \hline
		\textbf{Apache Ambari}     &    25.59     \\ \hline
		\textbf{Apache Nifi}       &    18.92     \\ \hline
		\textbf{Apache Tiles}      &    11.37     \\ \hline
		\textbf{Apache Shiro}      &    31.03     \\ \hline
		\textbf{Apache Usergrid}   &    26.90     \\ \hline
		\textbf{Apache Nutch}      &    32.11     \\ \hline
		\textbf{Apache Zookeeper}  &    26.70     \\ \hline
		\textbf{Apache Mina}       &    16.78     \\ \hline
		\textbf{Apache Cxf}        &    22.25     \\ \hline
		\textbf{Apache CloudStack} &    27.91     \\ \hline
		\textbf{Apache Oozie}      &    21.88     \\ \hline
		\textbf{Apache Kylin}      &    22.54     \\ \hline
		\textbf{Apache Flink}      &    15.37     \\ \hline
	\end{tabular}
	\caption{Percentage of overlap between God and SATD files of the analyzed projects.}
	\label{ch4_amount_of_overlap}
\end{table}

\pagebreak



\section{Threats to Validity}
\label{chap4:sec:threats_to_validity}


Threats to \textbf{internal validity} concern any factors that could have confounded our study results. Since developers might not think to declare the introduction of a technical debt in the first place, or remove the corresponding comment after eliminating a technical debt, one candidate is the use of source code comments. Every time the code and comment do not undergo a change simultaneously, the source code comments become a less and less accurate record. Despite this, Potdar and Shihab~\cite{ICSM_PotdarS14} found that in Eclipse code and comments were updated in tandem between 70\% and 90\% of the time. Another threat derives from comments intended to indicate SATD that do not correspond to any of the patterns Potdar and Shihab~\cite{ICSM_PotdarS14} compiled, which, owing to the flexibility of natural language, had to be analyzed manually. This technique is error-prone and somewhat subjective, in that developers could conceivably disagree as to which comments indicate SATD consistently. To mitigate these effects, we conducted manual inspections of all identified comments for each project in turn to certify that each contained one of the 62 patterns in \cite{ICSM_PotdarS14}. While we chose to identify any change containing at least one SATD file as an SATD change, we could have reserved this label for changes containing only SATD files. In our view, it is better not to restrict SATD changes in this way because sometimes all it takes is one SATD file to change several other files touched by the same change.


Threats to \textbf{external validity} concern the generalizability of our results. In order to optimize this, we analyzed 40 large open-source systems. Nonetheless, other systems should be analyzed to support the conclusions of this chapter, for one thing, because all the projects studied were written in Java, which limits programming language representation. Additionally, the projects studied were all developed by Apache, so systems developed by other companies could potentially run counter to our findings. Drawing on open-source projects means we have no guarantee that our results hold for industrial systems. Moreover, we focused on the relationship between SATD and god classes only, which means that other, unadmitted technical debt or code smells might have been overlooked. Nonetheless, studying all technical debt is beyond the scope of this thesis.

Threats to \textbf{construct validity} concern the extent to which indirect metrics do not accurately measure what they are intended to. Code metrics and thresholds were used to detect god classes as Marinescu \cite{marinescu2004detection} proposes, and while these have proven effective when applied in other studies, it has not been evaluated whether such strategies are suitable to use in all contexts. If not, findings will depend heavily on the particular metrics and thresholds specified for detection and varying these will alter our findings.

%and drew our data from the well-established, mature codebase of open-source software projects with well-commented source code

%We  believe the number of the analyzed systems  is sufficient enough to generalize our results. However,  we cannot be sure  that our findings  will be valid for other domains, applications,  programming languages, or proprietary  projects.

%Furthermore, we focused on SATD and god classes relationship only, which means that we do not cover all technical debt and therefore there may be other technical debt that is not self-admitted or a code smell. Studying all technical debt is beyond the scope of this thesis.





\section{Conclusion}
\label{chap4:sec:conclusion}


The software development community stigmatizes technical debt, even though it still lacks adequate evidence to formalize its adverse effects on software quality. Accordingly, the empirical study we present in this chapter seeks to identify in what ways god classes and self-admitted technical debt detract from quality. God classes centralize the workload of trivial classes and perform tasks using their data in violation of the object-oriented design principle stipulating one task per class. Self-admitted technical debt encompasses bugs that develop over time as a result of resorting to quick fixes that ``do the job" for the deadline and defer associated costs which could jeopardize the code in the long run. We identify god classes by employing Marinescu's \cite{marinescu2004detection} object-oriented metric thresholds and \SATD by locating source code comments that match the SATD indicator patterns in \cite{ICSM_PotdarS14}.


Three correlations allowed us to dissect the relationship between god classes and software quality: (i) whether god files have more defects than non-god files, (ii) whether god changes introduce future defects and (iii) whether god changes are more difficult to perform. Likewise, we examine the impact of \SATD on quality by determining (i) whether SATD files have more defects than non-SATD files, (ii) whether SATD changes introduce future defects and (iii) whether SATD-related changes are more difficult to perform. We measured change difficulty for both god classes and \SATD in terms of the amount of churn, numbers of files and modified modules in a change and entropy. After dealing with god and SATD files and changes separately, we assess (iv) to what extent the metric- and comment-based approaches overlap.


In the end, we found that (i) there is no dependable trend between god classes or \SATD and defects: three exceptional projects revealed more corrective changes in SATD files than in non-SATD files; (ii) a trend did surface, however, in that both god changes and SATD changes are more correlated with the introduction of future defects and (iii) more difficult to perform than non-god and non-SATD changes. As for the overlap between the two approaches, we learned that (iv) the metric- and comment-based approaches identify the same sources of technical debt in 11\% to 34\% of cases.

Our study imparts that although god classes and technical debt may have detrimental effects, these imply nothing with respect to defects \textit{per se}, but increase the number of defect-inducing changes and make the system more difficult to change in the future. 